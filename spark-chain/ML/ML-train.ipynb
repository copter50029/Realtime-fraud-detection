{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c60ca03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.9959\n",
      "Test Recall: 0.9959\n",
      "Test Accuracy: 0.9959\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Feature engineering in pandas ---\n",
    "data = pd.read_csv(\"fraudTrain.csv\")\n",
    "data['dob'] = pd.to_datetime(data['dob'], errors='coerce')\n",
    "data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], errors='coerce')\n",
    "data['age'] = data['trans_date_trans_time'].dt.year - data['dob'].dt.year\n",
    "\n",
    "def age_group(age):\n",
    "    if age < 18:\n",
    "        return 'Teen'\n",
    "    elif age < 25:\n",
    "        return 'Young Adult'\n",
    "    elif age < 35:\n",
    "        return 'Adult'\n",
    "    elif age < 50:\n",
    "        return 'Middle-aged'\n",
    "    elif age < 65:\n",
    "        return 'Senior'\n",
    "    else:\n",
    "        return 'Elderly'\n",
    "\n",
    "data['age_group'] = data['age'].apply(age_group)\n",
    "data['day'] = data['trans_date_trans_time'].dt.day\n",
    "data['month'] = data['trans_date_trans_time'].dt.month\n",
    "data['year'] = data['trans_date_trans_time'].dt.year\n",
    "data['hour'] = data['trans_date_trans_time'].dt.hour\n",
    "data['minute'] = data['trans_date_trans_time'].dt.minute\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "data['distance_km'] = haversine(data['lat'], data['long'], data['merch_lat'], data['merch_long'])\n",
    "\n",
    "def distance_category(dist):\n",
    "    if dist < 0.5:\n",
    "        return 'Very Near'\n",
    "    elif dist < 3:\n",
    "        return 'Near'\n",
    "    elif dist < 15:\n",
    "        return 'Moderate'\n",
    "    elif dist < 50:\n",
    "        return 'Far'\n",
    "    else:\n",
    "        return 'Very Far'\n",
    "\n",
    "data['distance_group'] = data['distance_km'].apply(distance_category)\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Unnamed: 0', 'first', 'last', 'street', 'city', 'state', 'zip',\n",
    "    'dob', 'cc_num', 'trans_num', 'unix_time',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long', 'trans_date_trans_time',\n",
    "]\n",
    "data.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# --- Sample and split ---\n",
    "sample_data = data.sample(frac=0.005, random_state=42)\n",
    "X = sample_data.drop(columns=['is_fraud'])\n",
    "y = sample_data['is_fraud']\n",
    "X['label'] = y.values\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.15, random_state=42)\n",
    "\n",
    "# --- Spark session ---\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "train_df = spark.createDataFrame(X_train)\n",
    "test_df = spark.createDataFrame(X_test)\n",
    "\n",
    "# --- Identify columns ---\n",
    "cat_cols = [field for (field, dtype) in train_df.dtypes if dtype == \"string\"]\n",
    "num_cols = [field for (field, dtype) in train_df.dtypes if dtype in [\"double\", \"int\"] and field != \"label\"]\n",
    "\n",
    "# --- Pipeline stages ---\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\", handleInvalid=\"keep\") for col in cat_cols]\n",
    "assembler_inputs = [col+\"_idx\" for col in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features_vec\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\")\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler, gbt])\n",
    "\n",
    "# --- Train pipeline ---\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# --- Evaluate ---\n",
    "predictions = pipeline_model.transform(test_df)\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "recall_score_model = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedRecall')\n",
    "precision_score_model = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedPrecision')\n",
    "\n",
    "accuracy = multi_evaluator.evaluate(predictions)\n",
    "recall = recall_score_model.evaluate(predictions)\n",
    "precision = precision_score_model.evaluate(predictions)\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# --- Save pipeline model ---\n",
    "pipeline_model.save(\"pipeline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c6a0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truePositiveRateByLabel: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 38234)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "True_score_model = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',metricName=\"truePositiveRateByLabel\")\n",
    "True_positive = True_score_model.evaluate(predictions)\n",
    "print(f\"truePositiveRateByLabel: {True_positive:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
