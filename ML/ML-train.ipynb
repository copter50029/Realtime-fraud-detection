{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Feature engineering in pandas ---\n",
    "data = pd.read_csv(\"fraudTrain.csv\")\n",
    "data['dob'] = pd.to_datetime(data['dob'], errors='coerce')\n",
    "data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], errors='coerce')\n",
    "data['age'] = data['trans_date_trans_time'].dt.year - data['dob'].dt.year\n",
    "\n",
    "def age_group(age):\n",
    "    if age < 18:\n",
    "        return 'Teen'\n",
    "    elif age < 25:\n",
    "        return 'Young Adult'\n",
    "    elif age < 35:\n",
    "        return 'Adult'\n",
    "    elif age < 50:\n",
    "        return 'Middle-aged'\n",
    "    elif age < 65:\n",
    "        return 'Senior'\n",
    "    else:\n",
    "        return 'Elderly'\n",
    "\n",
    "data['age_group'] = data['age'].apply(age_group)\n",
    "data['day'] = data['trans_date_trans_time'].dt.day\n",
    "data['month'] = data['trans_date_trans_time'].dt.month\n",
    "data['year'] = data['trans_date_trans_time'].dt.year\n",
    "data['hour'] = data['trans_date_trans_time'].dt.hour\n",
    "data['minute'] = data['trans_date_trans_time'].dt.minute\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "data['distance_km'] = haversine(data['lat'], data['long'], data['merch_lat'], data['merch_long'])\n",
    "\n",
    "def distance_category(dist):\n",
    "    if dist < 0.5:\n",
    "        return 'Very Near'\n",
    "    elif dist < 3:\n",
    "        return 'Near'\n",
    "    elif dist < 15:\n",
    "        return 'Moderate'\n",
    "    elif dist < 50:\n",
    "        return 'Far'\n",
    "    else:\n",
    "        return 'Very Far'\n",
    "\n",
    "data['distance_group'] = data['distance_km'].apply(distance_category)\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Unnamed: 0', 'first', 'last', 'street', 'city', 'state', 'zip',\n",
    "    'dob', 'cc_num', 'trans_num', 'unix_time',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long', 'trans_date_trans_time',\n",
    "]\n",
    "data.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# --- Sample and split ---\n",
    "sample_data = data.sample(frac=0.005, random_state=42)\n",
    "X = sample_data.drop(columns=['is_fraud'])\n",
    "y = sample_data['is_fraud']\n",
    "X['label'] = y.values\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.15, random_state=42)\n",
    "\n",
    "# --- Spark session ---\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "train_df = spark.createDataFrame(X_train)\n",
    "test_df = spark.createDataFrame(X_test)\n",
    "\n",
    "# --- Identify columns ---\n",
    "cat_cols = [field for (field, dtype) in train_df.dtypes if dtype == \"string\"]\n",
    "num_cols = [field for (field, dtype) in train_df.dtypes if dtype in [\"double\", \"int\"] and field != \"label\"]\n",
    "\n",
    "# --- Pipeline stages ---\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\", handleInvalid=\"keep\") for col in cat_cols]\n",
    "assembler_inputs = [col+\"_idx\" for col in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features_vec\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\")\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler, gbt])\n",
    "\n",
    "# --- Train pipeline ---\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# --- Evaluate ---\n",
    "predictions = pipeline_model.transform(test_df)\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "recall_score_model = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedRecall')\n",
    "precision_score_model = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedPrecision')\n",
    "\n",
    "accuracy = multi_evaluator.evaluate(predictions)\n",
    "recall = recall_score_model.evaluate(predictions)\n",
    "precision = precision_score_model.evaluate(predictions)\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# --- Save pipeline model ---\n",
    "pipeline_model.save(\"pipeline_model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
